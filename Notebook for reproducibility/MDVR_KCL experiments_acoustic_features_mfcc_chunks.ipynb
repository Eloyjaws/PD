{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c6006b",
   "metadata": {},
   "source": [
    "### MDVR-KCL experiments using the segmented wav files. (13 mfcc + acoustic features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9adc993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c4c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MDVR_split_on_silence_500ms_all_features_v2.csv\")\n",
    "# df = pd.read_csv(\"MDVR_all_features_chunk_5000ms_v2.csv\")\n",
    "df.drop(['voiceID'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3de2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows wth na\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f6b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate dependent and independent variable for acoustic features only\n",
    "X = df.iloc[:, :-1]\n",
    "df_X = df.iloc[:, :-1]\n",
    "df_Y = df.iloc[:,-1]\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb599d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9b7e2",
   "metadata": {},
   "source": [
    "#### K Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf25290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_eval(alg):\n",
    "    df_X = df.iloc[:, :-1].values\n",
    "    df_Y = df.iloc[:,-1].values\n",
    "    \n",
    "    df_X = pd.DataFrame(df_X)\n",
    "    df_Y = pd.DataFrame(df_Y)\n",
    "\n",
    "    parts = 5\n",
    "    kfold = KFold(parts, shuffle=True)\n",
    "    \n",
    "    accuracy_list = []\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    precision_list =[]\n",
    "    f1_knn_list = []\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        accuracy_total = 0\n",
    "        sensitivity_total =0\n",
    "        specificity_total=0\n",
    "        precision_total=0\n",
    "        f1_knn_total=0\n",
    "        \n",
    "        for train, test in kfold.split(df_X,df_Y):\n",
    "            Xtrain = df_X.iloc[train, :]\n",
    "            #print(Xtrain)\n",
    "            Ytrain = df_Y.iloc[train, :]\n",
    "            #print(Ytrain)\n",
    "            Xtest = df_X.iloc[test, :]\n",
    "            Ytest = df_Y.iloc[test, :]\n",
    "            #scale\n",
    "            sc = MinMaxScaler()\n",
    "            Xtrain = sc.fit_transform(Xtrain)\n",
    "            Xtest = sc.transform(Xtest)\n",
    "            \n",
    "            #modelling\n",
    "            if (alg == \"LR\"):\n",
    "                model = LogisticRegression(max_iter=3000)\n",
    "            elif (alg == \"GB\"):\n",
    "                model = GradientBoostingClassifier(learning_rate= 0.5, max_depth=1,n_estimators=3)\n",
    "            elif (alg == \"KNN\"):\n",
    "                model = KNeighborsClassifier(n_neighbors = 6, p=1, weights=\"distance\", leaf_size=1, algorithm=\"auto\")\n",
    "            elif (alg == \"SVM\"):\n",
    "                model = svm.SVC(C=100, gamma=1, kernel=\"rbf\")\n",
    "            elif (alg == \"DT\"):\n",
    "                model = tree.DecisionTreeClassifier()\n",
    "            elif (alg == \"NB\"):\n",
    "                model =  GaussianNB()\n",
    "            elif (alg == \"RF\"):\n",
    "                model = RandomForestClassifier(max_features=\"auto\", min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "                \n",
    "            model.fit(Xtrain, Ytrain.values.ravel())\n",
    "            y_pred = model.predict(Xtest)\n",
    "\n",
    "            conf_matrix = confusion_matrix(Ytest, y_pred)\n",
    "            #print(conf_matrix)\n",
    "            TN = conf_matrix[0][0]\n",
    "            FP = conf_matrix[0][1]\n",
    "            FN = conf_matrix[1][0]\n",
    "            TP = conf_matrix[1][1]\n",
    "            #print(TN, \", \", FP, \", \", FN, \", \", TP)\n",
    "\n",
    "            accuracy = ((TP + TN) / (TP + TN + FP + FN)) * 100\n",
    "            sensitivity = (TP/(TP+FN)) * 100 #recall\n",
    "            specificity = (TN/(TN + FP)) * 100\n",
    "            precision = (TP/(TP+FP)) * 100\n",
    "            f1_knn = 2 *((sensitivity * precision)/(sensitivity + precision))\n",
    "            \n",
    "            #sum it up\n",
    "            accuracy_total += accuracy\n",
    "            sensitivity_total += sensitivity\n",
    "            specificity_total += specificity\n",
    "            precision_total += precision\n",
    "            f1_knn_total += f1_knn\n",
    "            \n",
    "            #avg\n",
    "            accuracy_mean = accuracy_total/parts\n",
    "            sensitivity_mean = sensitivity_total/parts\n",
    "            specificity_mean = specificity_total/parts\n",
    "            precision_mean = precision_total/parts\n",
    "            f1_mean = f1_knn_total/parts\n",
    "            \n",
    "        #print(\"Loop \", i, \"\\n Mean Accuracy: \", accuracy_total/parts,\n",
    "                           #  \"\\n Mean sensitivity: \",sensitivity_total/parts,\n",
    "                            # \"\\n Mean Specificity: \", specificity_total/parts,\n",
    "                             #\"\\n Mean Precision: \",precision_total/parts,\n",
    "                             #\"\\n Mean f1: \",f1_knn_total/parts)\n",
    "        accuracy_list.append(accuracy_mean)\n",
    "        sensitivity_list.append(sensitivity_mean)\n",
    "        specificity_list.append(specificity_mean)\n",
    "        precision_list.append(precision_mean)\n",
    "        f1_knn_list.append(f1_mean)\n",
    "\n",
    "    print(\"\\n==================================================\\n\")\n",
    "    print(alg)\n",
    "    # print(\"Accuracy for the 10 iterations: \",  accuracy_list) #mean accuracy acros the 6 folds for each iteration\n",
    "    print(\"Average accuracy: \", np.mean(accuracy_list), \"\\n\")\n",
    "    \n",
    "    # print(\"Sensitivity for the 10 iterations: \",  sensitivity_list) #mean accuracy acros the 6 folds for each iteration\n",
    "    print(\"Average sensitivity: \", np.mean(sensitivity_list), \"\\n\")\n",
    "        \n",
    "    # print(\"Specificity for the 10 iterations: \",  specificity_list) #mean accuracy acros the 6 folds for each iteration\n",
    "    print(\"Average specificity: \", np.mean(specificity_list), \"\\n\")\n",
    "    \n",
    "    # print(\"Precision for the 10 iterations: \",  precision_list) \n",
    "    print(\"Average precision: \", np.mean(precision_list), \"\\n\")\n",
    "    \n",
    "    # print(\"F1  score for the 10 iterations: \",  f1_knn_list) \n",
    "    print(\"Average f1 score: \", np.mean(f1_knn_list), \"\\n\")\n",
    "    \n",
    "       \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66aae3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "KNN\n",
      "Average accuracy:  90.13552188552188 \n",
      "\n",
      "Average sensitivity:  87.59613958614106 \n",
      "\n",
      "Average specificity:  92.02908390546578 \n",
      "\n",
      "Average precision:  88.99450768965309 \n",
      "\n",
      "Average f1 score:  88.25490700148063 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "DT\n",
      "Average accuracy:  78.83858858858859 \n",
      "\n",
      "Average sensitivity:  74.80866677777696 \n",
      "\n",
      "Average specificity:  81.85356641689835 \n",
      "\n",
      "Average precision:  75.29584417718159 \n",
      "\n",
      "Average f1 score:  74.93129265270099 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "SVM\n",
      "Average accuracy:  91.2292064792065 \n",
      "\n",
      "Average sensitivity:  89.57820607386451 \n",
      "\n",
      "Average specificity:  92.47851094916084 \n",
      "\n",
      "Average precision:  89.764576835457 \n",
      "\n",
      "Average f1 score:  89.62870412497617 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "NB\n",
      "Average accuracy:  73.67890617890619 \n",
      "\n",
      "Average sensitivity:  65.79865668049877 \n",
      "\n",
      "Average specificity:  79.47283477129622 \n",
      "\n",
      "Average precision:  70.21947051697559 \n",
      "\n",
      "Average f1 score:  67.87412185025751 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "LR\n",
      "Average accuracy:  79.07502957502956 \n",
      "\n",
      "Average sensitivity:  69.57808448589105 \n",
      "\n",
      "Average specificity:  86.08859621601718 \n",
      "\n",
      "Average precision:  78.59863492233748 \n",
      "\n",
      "Average f1 score:  73.74274279756676 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "GB\n",
      "Average accuracy:  73.97092547092547 \n",
      "\n",
      "Average sensitivity:  48.77961299587521 \n",
      "\n",
      "Average specificity:  92.60094380743188 \n",
      "\n",
      "Average precision:  83.03424622856754 \n",
      "\n",
      "Average f1 score:  61.20391795304855 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "RF\n",
      "Average accuracy:  86.6911911911912 \n",
      "\n",
      "Average sensitivity:  77.82359378985333 \n",
      "\n",
      "Average specificity:  93.23472628840257 \n",
      "\n",
      "Average precision:  89.51119625645913 \n",
      "\n",
      "Average f1 score:  83.17586419156538 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold_eval(\"KNN\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"DT\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"SVM\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"NB\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"LR\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"GB\")\n",
    "\n",
    "print(\"\\n\")\n",
    "kfold_eval(\"RF\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfab0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'ML Alg':['KNN', 'SVM', 'Decision Tree', 'Naive Bayes', 'Random Forest'],\n",
    "#         'Accuracy':[accuracy_knn, accuracy_svm, accuracy_dt, accuracy_nb, accuracy_rf]}\n",
    "  \n",
    "# # Create DataFrame\n",
    "# results = pd.DataFrame(data)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa1f99-6506-4720-ad56-7a154373e08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
